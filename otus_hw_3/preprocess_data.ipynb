{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c802c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydoop\n",
    "# !pip install boto3\n",
    "# !pip install pyspark==3.0.2\n",
    "# !pip install findspark\n",
    "# !pip install sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d2a02",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec836753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sh\n",
    "import boto3\n",
    "import findspark\n",
    "\n",
    "import pydoop.hdfs as hdfs\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172fce2",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd38c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "# findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "257a0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .config(conf=SparkConf())\n",
    "         .appName(\"local\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True) \n",
    "\n",
    "schema = StructType([StructField(\"transaction_id\", LongType(), True),\n",
    "                     StructField(\"tx_datetime\", TimestampType(), True),\n",
    "                     StructField(\"customer_id\", LongType(), True),\n",
    "                     StructField(\"terminal_id\", LongType(), True),\n",
    "                     StructField(\"tx_amount\", DoubleType(), True),\n",
    "                     StructField(\"tx_time_seconds\", LongType(), True),\n",
    "                     StructField(\"tx_time_days\", LongType(), True),\n",
    "                     StructField(\"tx_fraud\", LongType(), True),\n",
    "                     StructField(\"tx_fraud_scenario\", LongType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dbfd0",
   "metadata": {},
   "source": [
    "# File Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f72e86",
   "metadata": {},
   "source": [
    "### Init vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99e3a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://rc1a-dataproc-m-r02wxfzuv2ttjl1t.mdb.yandexcloud.net\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey fs.defaultFS # получить хост hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e657eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_host = 'hdfs://rc1a-dataproc-m-r02wxfzuv2ttjl1t.mdb.yandexcloud.net'\n",
    "path = '/user/ubuntu/'\n",
    "target_s3 = 's3a://mlops-bucket-parquet-20231030/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9296783",
   "metadata": {},
   "source": [
    "### Files list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109e7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 items\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2023-11-06 20:20 .sparkStaging\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2807409271 2023-10-31 10:04 2019-08-22.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2854479008 2023-10-31 10:05 2019-09-21.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2895460543 2023-10-31 10:05 2019-10-21.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2939120942 2023-10-31 10:04 2019-11-20.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995462277 2023-10-31 10:02 2019-12-20.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2994906767 2023-10-31 10:09 2020-01-19.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995431240 2023-10-31 10:01 2020-02-18.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995176166 2023-10-31 10:02 2020-03-19.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2996034632 2023-10-31 10:02 2020-04-18.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995666965 2023-10-31 10:06 2020-05-18.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2994699401 2023-10-31 10:02 2020-06-17.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995810010 2023-10-31 10:02 2020-07-17.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995995152 2023-10-31 10:07 2020-08-16.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995778382 2023-10-31 10:01 2020-09-15.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995868596 2023-10-31 10:04 2020-10-15.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995467533 2023-10-31 10:04 2020-11-14.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2994761624 2023-10-31 10:02 2020-12-14.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995390576 2023-10-31 10:05 2021-01-13.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995780517 2023-10-31 10:01 2021-02-12.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995191659 2023-10-31 10:02 2021-03-14.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995446495 2023-10-31 10:04 2021-04-13.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3029170975 2023-10-31 10:04 2021-05-13.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042691991 2023-10-31 10:06 2021-06-12.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3041980335 2023-10-31 10:06 2021-07-12.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042662187 2023-10-31 10:07 2021-08-11.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042455173 2023-10-31 10:07 2021-09-10.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042424238 2023-10-31 10:08 2021-10-10.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042358698 2023-10-31 10:07 2021-11-09.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042923985 2023-10-31 10:08 2021-12-09.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042868087 2023-10-31 10:09 2022-01-08.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3043148790 2023-10-31 10:07 2022-02-07.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042312191 2023-10-31 10:08 2022-03-09.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3041973966 2023-10-31 10:09 2022-04-08.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3073760161 2023-10-31 10:09 2022-05-08.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3089378246 2023-10-31 10:03 2022-06-07.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3089589719 2023-10-31 10:08 2022-07-07.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3090000257 2023-10-31 10:05 2022-08-06.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3089390874 2023-10-31 10:06 2022-09-05.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3109468067 2023-10-31 10:08 2022-10-05.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3136657969 2023-10-31 10:06 2022-11-04.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167d4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfsdir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a7fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [ line.rsplit(None,1)[-1] for line in sh.hdfs('dfs','-ls',hdfsdir).split('\\n') if len(line.rsplit(None,1))][1:]\n",
    "filelist = [f for f in filelist if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d756033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cede33",
   "metadata": {},
   "source": [
    "## One File Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c35fe23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-08-22.txt'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = filelist[0]\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3699f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema)\\\n",
    "                .option(\"comment\", \"#\")\\\n",
    "                .option(\"header\", False)\\\n",
    "                .csv(f'{hdfs_host}{path}{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbedcd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>transaction_id</th><th>tx_datetime</th><th>customer_id</th><th>terminal_id</th><th>tx_amount</th><th>tx_time_seconds</th><th>tx_time_days</th><th>tx_fraud</th><th>tx_fraud_scenario</th></tr>\n",
       "<tr><td>0</td><td>2019-08-22 06:51:03</td><td>0</td><td>711</td><td>70.91</td><td>24663</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>1</td><td>2019-08-22 05:10:37</td><td>0</td><td>0</td><td>90.55</td><td>18637</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>2</td><td>2019-08-22 19:05:33</td><td>0</td><td>753</td><td>35.38</td><td>68733</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>3</td><td>2019-08-22 07:21:33</td><td>0</td><td>0</td><td>80.41</td><td>26493</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>4</td><td>2019-08-22 09:06:17</td><td>1</td><td>981</td><td>102.83</td><td>32777</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>5</td><td>2019-08-22 18:41:25</td><td>3</td><td>205</td><td>34.2</td><td>67285</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>6</td><td>2019-08-22 03:12:21</td><td>3</td><td>0</td><td>47.2</td><td>11541</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>7</td><td>2019-08-22 22:36:40</td><td>6</td><td>809</td><td>139.39</td><td>81400</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>8</td><td>2019-08-22 17:23:29</td><td>7</td><td>184</td><td>87.24</td><td>62609</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>9</td><td>2019-08-22 21:09:37</td><td>8</td><td>931</td><td>61.7</td><td>76177</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>10</td><td>2019-08-22 11:32:42</td><td>10</td><td>663</td><td>40.71</td><td>41562</td><td>0</td><td>1</td><td>2</td></tr>\n",
       "<tr><td>11</td><td>2019-08-22 03:09:26</td><td>10</td><td>770</td><td>63.91</td><td>11366</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>12</td><td>2019-08-22 15:47:54</td><td>10</td><td>0</td><td>58.89</td><td>56874</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>13</td><td>2019-08-22 21:59:20</td><td>10</td><td>649</td><td>89.24</td><td>79160</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>14</td><td>2019-08-22 20:55:13</td><td>11</td><td>380</td><td>9.89</td><td>75313</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>15</td><td>2019-08-22 16:39:03</td><td>11</td><td>337</td><td>83.36</td><td>59943</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>16</td><td>2019-08-22 23:15:07</td><td>11</td><td>973</td><td>35.12</td><td>83707</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>17</td><td>2019-08-22 07:39:45</td><td>12</td><td>9</td><td>74.0</td><td>27585</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>18</td><td>2019-08-22 05:35:39</td><td>12</td><td>745</td><td>108.63</td><td>20139</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "<tr><td>19</td><td>2019-08-22 10:29:16</td><td>12</td><td>9</td><td>84.45</td><td>37756</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
       "|transaction_id|        tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
       "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
       "|             0|2019-08-22 06:51:03|          0|        711|    70.91|          24663|           0|       0|                0|\n",
       "|             1|2019-08-22 05:10:37|          0|          0|    90.55|          18637|           0|       0|                0|\n",
       "|             2|2019-08-22 19:05:33|          0|        753|    35.38|          68733|           0|       0|                0|\n",
       "|             3|2019-08-22 07:21:33|          0|          0|    80.41|          26493|           0|       0|                0|\n",
       "|             4|2019-08-22 09:06:17|          1|        981|   102.83|          32777|           0|       0|                0|\n",
       "|             5|2019-08-22 18:41:25|          3|        205|     34.2|          67285|           0|       0|                0|\n",
       "|             6|2019-08-22 03:12:21|          3|          0|     47.2|          11541|           0|       0|                0|\n",
       "|             7|2019-08-22 22:36:40|          6|        809|   139.39|          81400|           0|       0|                0|\n",
       "|             8|2019-08-22 17:23:29|          7|        184|    87.24|          62609|           0|       0|                0|\n",
       "|             9|2019-08-22 21:09:37|          8|        931|     61.7|          76177|           0|       0|                0|\n",
       "|            10|2019-08-22 11:32:42|         10|        663|    40.71|          41562|           0|       1|                2|\n",
       "|            11|2019-08-22 03:09:26|         10|        770|    63.91|          11366|           0|       0|                0|\n",
       "|            12|2019-08-22 15:47:54|         10|          0|    58.89|          56874|           0|       0|                0|\n",
       "|            13|2019-08-22 21:59:20|         10|        649|    89.24|          79160|           0|       0|                0|\n",
       "|            14|2019-08-22 20:55:13|         11|        380|     9.89|          75313|           0|       0|                0|\n",
       "|            15|2019-08-22 16:39:03|         11|        337|    83.36|          59943|           0|       0|                0|\n",
       "|            16|2019-08-22 23:15:07|         11|        973|    35.12|          83707|           0|       0|                0|\n",
       "|            17|2019-08-22 07:39:45|         12|          9|     74.0|          27585|           0|       0|                0|\n",
       "|            18|2019-08-22 05:35:39|         12|        745|   108.63|          20139|           0|       0|                0|\n",
       "|            19|2019-08-22 10:29:16|         12|          9|    84.45|          37756|           0|       0|                0|\n",
       "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8e88600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46988418"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin_size = df.count()\n",
    "print('Начальных размер DF: ', begin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "305fa0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|transaction_id|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+--------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|             0|          0|          0|        0|              0|           0|       0|                0|\n",
      "+--------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Количество пропущенных значений по столбцам\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns if c != 'tx_datetime']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c8ebd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление строк с пустыми записями\n",
    "df = df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd67e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление дубликатов по id транзакции\n",
    "df = df.dropDuplicates(['transaction_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50fe0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_size = df.count()\n",
    "print('Итоговый размер DF: ', end_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba6120c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дельта изменения количества записей:  281\n"
     ]
    }
   ],
   "source": [
    "print('Дельта изменения количества записей: ', begin_size - end_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c12e5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(f\"{target_s3}{filename}.parquet\", mode=\"overwrite\")\n",
    "# df.write.parquet(f'{hdfs_host}{path}data_parquet/2022-11-04.parquet', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91094cd2",
   "metadata": {},
   "source": [
    "## Process ALl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb00d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b213bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_ID = 'YCAJEvTr4KCBuJY-36m1fcaqj'\n",
    "SECRET = 'YCPenldqUFHEzPrRb5SunkP-YRo0QPfJakAzPyt4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cda160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_exist(filename):\n",
    "    '''\n",
    "    Check for file existance.\n",
    "    \n",
    "    NOW IS NOT WORKING WELL\n",
    "    '''\n",
    "    \n",
    "    session = boto3.session.Session(aws_access_key_id=KEY_ID, aws_secret_access_key=SECRET)\n",
    "    s3 = session.client(\n",
    "        service_name='s3',\n",
    "        endpoint_url='https://storage.yandexcloud.net'\n",
    "    )\n",
    "    filelist_s3 = [key['Key'].split('.')[0] for key in s3.list_objects(Bucket='mlops-bucket-parquet-20231030')['Contents']]\n",
    "    if filename in filelist_s3:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333d45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename:  2019-08-22.txt\n",
      "Начальных размер DF:  46988418\n",
      "Итоговый размер DF:  46988137\n",
      "Дельта изменения количества записей:  281\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2019-09-21.txt\n",
      "Начальных размер DF:  46994586\n",
      "Итоговый размер DF:  46993705\n",
      "Дельта изменения количества записей:  881\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2019-10-21.txt\n",
      "Начальных размер DF:  46994432\n",
      "Итоговый размер DF:  46993895\n",
      "Дельта изменения количества записей:  537\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2019-11-20.txt\n",
      "Начальных размер DF:  46992239\n",
      "Итоговый размер DF:  46990123\n",
      "Дельта изменения количества записей:  2116\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2019-12-20.txt\n",
      "Начальных размер DF:  46994937\n",
      "Итоговый размер DF:  46993787\n",
      "Дельта изменения количества записей:  1150\n"
     ]
    }
   ],
   "source": [
    "begin_size_all = 0\n",
    "end_size_all = 0\n",
    "\n",
    "for filename in filelist:\n",
    "    print('Filename: ', filename)\n",
    "    \n",
    "    if check_file_exist(filename):\n",
    "        print(filename, ' already exist')\n",
    "        print()\n",
    "        continue\n",
    "        \n",
    "    df = spark.read.schema(schema)\\\n",
    "                .option(\"comment\", \"#\")\\\n",
    "                .option(\"header\", False)\\\n",
    "                .csv(f'{hdfs_host}{path}{filename}')\n",
    "    begin_size = df.count()\n",
    "    print('Начальных размер DF: ', begin_size)\n",
    "    df = df.na.drop(\"any\")\n",
    "    df = df.dropDuplicates(['transaction_id'])\n",
    "    end_size = df.count()\n",
    "    print('Итоговый размер DF: ', end_size)\n",
    "    print('Дельта изменения количества записей: ', begin_size - end_size)\n",
    "    \n",
    "    df.write.parquet(f\"{target_s3}{filename}.parquet\", mode=\"overwrite\")\n",
    "    print('Файл сохранен.')\n",
    "    begin_size_all += begin_size\n",
    "    end_size_all += end_size\n",
    "    print()\n",
    "    \n",
    "print('Finish!')\n",
    "print('Всего очищенных записей: ', begin_size_all - end_size_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be521958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hadoop fs -mkdir ./data_parquet\n",
    "# !hadoop dfs -rm -r hdfs://rc1a-dataproc-m-r02wxfzuv2ttjl1t.mdb.yandexcloud.net/user/ubuntu/data_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d167c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_bucket = ['2019-08-22.parquet',\n",
    "#              '2019-09-21.parquet', \n",
    "#              '2019-10-21.parquet', \n",
    "#              '2019-11-20.parquet', \n",
    "#              '2019-12-20.parquet', \n",
    "#              '2020-01-19.parquet', \n",
    "#              '2020-02-18.parquet', \n",
    "#              '2020-03-19.parquet', \n",
    "#              '2020-04-18.parquet', \n",
    "#              '2020-05-18.parquet',\n",
    "#              '2020-06-17.parquet',\n",
    "#             '2020-07-17.parquet',\n",
    "#             '2020-08-16.parquet',\n",
    "#             '2020-09-15.parquet',\n",
    "#             '2020-10-15.parquet',\n",
    "#             '2020-11-14.parquet',\n",
    "#             '2020-12-14.parquet', \n",
    "#             '2021-01-13.parquet', \n",
    "#             '2021-02-12.parquet', \n",
    "#             '2021-03-14.parquet', \n",
    "#             '2021-04-13.parquet', \n",
    "#             '2021-05-13.parquet', \n",
    "#             '2021-06-12.parquet', \n",
    "#             '2021-07-12.parquet', \n",
    "#             '2021-08-11.parquet',\n",
    "#             '2021-09-10.parquet', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac890b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename:  2019-08-22.txt\n",
      "2019-08-22  already exist\n",
      "\n",
      "Filename:  2019-09-21.txt\n",
      "2019-09-21  already exist\n",
      "\n",
      "Filename:  2019-10-21.txt\n",
      "2019-10-21  already exist\n",
      "\n",
      "Filename:  2019-11-20.txt\n",
      "2019-11-20  already exist\n",
      "\n",
      "Filename:  2019-12-20.txt\n",
      "2019-12-20  already exist\n",
      "\n",
      "Filename:  2020-01-19.txt\n",
      "2020-01-19  already exist\n",
      "\n",
      "Filename:  2020-02-18.txt\n",
      "2020-02-18  already exist\n",
      "\n",
      "Filename:  2020-03-19.txt\n",
      "2020-03-19  already exist\n",
      "\n",
      "Filename:  2020-04-18.txt\n",
      "2020-04-18  already exist\n",
      "\n",
      "Filename:  2020-05-18.txt\n",
      "2020-05-18  already exist\n",
      "\n",
      "Filename:  2020-06-17.txt\n",
      "2020-06-17  already exist\n",
      "\n",
      "Filename:  2020-07-17.txt\n",
      "2020-07-17  already exist\n",
      "\n",
      "Filename:  2020-08-16.txt\n",
      "2020-08-16  already exist\n",
      "\n",
      "Filename:  2020-09-15.txt\n",
      "2020-09-15  already exist\n",
      "\n",
      "Filename:  2020-10-15.txt\n",
      "2020-10-15  already exist\n",
      "\n",
      "Filename:  2020-11-14.txt\n",
      "2020-11-14  already exist\n",
      "\n",
      "Filename:  2020-12-14.txt\n",
      "2020-12-14  already exist\n",
      "\n",
      "Filename:  2021-01-13.txt\n",
      "2021-01-13  already exist\n",
      "\n",
      "Filename:  2021-02-12.txt\n",
      "2021-02-12  already exist\n",
      "\n",
      "Filename:  2021-03-14.txt\n",
      "2021-03-14  already exist\n",
      "\n",
      "Filename:  2021-04-13.txt\n",
      "2021-04-13  already exist\n",
      "\n",
      "Filename:  2021-05-13.txt\n",
      "2021-05-13  already exist\n",
      "\n",
      "Filename:  2021-06-12.txt\n",
      "2021-06-12  already exist\n",
      "\n",
      "Filename:  2021-07-12.txt\n",
      "2021-07-12  already exist\n",
      "\n",
      "Filename:  2021-08-11.txt\n",
      "2021-08-11  already exist\n",
      "\n",
      "Filename:  2021-09-10.txt\n",
      "2021-09-10  already exist\n",
      "\n",
      "Filename:  2021-10-10.txt\n",
      "Начальных размер DF:  46994093\n",
      "Итоговый размер DF:  46993819\n",
      "Дельта изменения количества записей:  274\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2021-11-09.txt\n",
      "Начальных размер DF:  46993018\n",
      "Итоговый размер DF:  46990869\n",
      "Дельта изменения количества записей:  2149\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2021-12-09.txt\n",
      "Начальных размер DF:  47001969\n",
      "Итоговый размер DF:  47001873\n",
      "Дельта изменения количества записей:  96\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-01-08.txt\n",
      "Начальных размер DF:  46997815\n",
      "Итоговый размер DF:  46996116\n",
      "Дельта изменения количества записей:  1699\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-02-07.txt\n",
      "Начальных размер DF:  47005087\n",
      "Итоговый размер DF:  47003555\n",
      "Дельта изменения количества записей:  1532\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-03-09.txt\n",
      "Начальных размер DF:  46992441\n",
      "Итоговый размер DF:  46991825\n",
      "Дельта изменения количества записей:  616\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-04-08.txt\n",
      "Начальных размер DF:  46987223\n",
      "Итоговый размер DF:  46986472\n",
      "Дельта изменения количества записей:  751\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-05-08.txt\n",
      "Начальных размер DF:  46993169\n",
      "Итоговый размер DF:  46992649\n",
      "Дельта изменения количества записей:  520\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-06-07.txt\n",
      "Начальных размер DF:  46992461\n",
      "Итоговый размер DF:  46988555\n",
      "Дельта изменения количества записей:  3906\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-07-07.txt\n",
      "Начальных размер DF:  46996444\n",
      "Итоговый размер DF:  46996073\n",
      "Дельта изменения количества записей:  371\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-08-06.txt\n",
      "Начальных размер DF:  47002232\n",
      "Итоговый размер DF:  47001923\n",
      "Дельта изменения количества записей:  309\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-09-05.txt\n",
      "Начальных размер DF:  46993904\n",
      "Итоговый размер DF:  46993333\n",
      "Дельта изменения количества записей:  571\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-10-05.txt\n",
      "Начальных размер DF:  46997187\n",
      "Итоговый размер DF:  46996500\n",
      "Дельта изменения количества записей:  687\n",
      "Файл сохранен.\n",
      "\n",
      "Filename:  2022-11-04.txt\n",
      "Начальных размер DF:  46998983\n",
      "Итоговый размер DF:  46996585\n",
      "Дельта изменения количества записей:  2398\n",
      "Файл сохранен.\n",
      "\n",
      "Finish!\n",
      "Всего очищенных записей:  15879\n"
     ]
    }
   ],
   "source": [
    "begin_size_all = 0\n",
    "end_size_all = 0\n",
    "\n",
    "for filename in filelist:\n",
    "    print('Filename: ', filename)\n",
    "    filename = filename.split('.')[0]\n",
    "    \n",
    "    if filename in [i.split('.')[0] for i in in_bucket]:\n",
    "        print(filename, ' already exist')\n",
    "        print()\n",
    "        continue\n",
    "        \n",
    "    df = spark.read.schema(schema)\\\n",
    "                .option(\"comment\", \"#\")\\\n",
    "                .option(\"header\", False)\\\n",
    "                .csv(f'{hdfs_host}{path}{filename}.txt')\n",
    "    begin_size = df.count()\n",
    "    print('Начальных размер DF: ', begin_size)\n",
    "    df = df.na.drop(\"any\") # очистка пустых значений\n",
    "    df = df.dropDuplicates(['transaction_id']) # очистка дублей\n",
    "    end_size = df.count()\n",
    "    print('Итоговый размер DF: ', end_size)\n",
    "    print('Дельта изменения количества записей: ', begin_size - end_size)\n",
    "    \n",
    "    df.write.parquet(f\"{target_s3}{filename}.parquet\", mode=\"overwrite\")\n",
    "#     df.write.parquet(f'{hdfs_host}{path}data_parquet/{filename}.parquet', mode=\"overwrite\")\n",
    "    print('Файл сохранен.')\n",
    "    begin_size_all += begin_size\n",
    "    end_size_all += end_size\n",
    "    print()\n",
    "    \n",
    "print('Finish!')\n",
    "print('Всего очищенных записей: ', begin_size_all - end_size_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90887c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46643"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 281 + 881 + 537 + 2116 + 1150 + 148 + 596 + 3344 + 317 + 278 + 1726 + 827 + 93 + 3481 + 2590 + 2259 + 402 + 160 + 2296 + 91 + 1752 + 825 + 799 + 566 + 2205 + 1071 + 247 + 2149 + 96 + 1699 + 1532 + 616 + 751 + 520 + 3906 + 371 + 309 + 571 + 687 + 2398\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b79eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество удаленных записей:  46643\n"
     ]
    }
   ],
   "source": [
    "print('Количество удаленных записей: ', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf3a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
